{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "cache_dir = \"../../models_weights/\"\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "cache_dir = \"../../models_weights/\"\n",
    "\n",
    "# Load the model and tokenizer with cache_dir\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=cache_dir, token=access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir, token=access_token)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set up an environment variable that will allow us to run the model evaluation. \n",
    "import os\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\"\"\"\n",
    "openai_humaneval dataset from Hugging Face. This dataset contains 164 Python programming problems and includes English natural text \n",
    "found in comments and docstrings. \n",
    "\"\"\"\n",
    "# Load HumanEval dataset\n",
    "human_eval = load_dataset(\"openai_humaneval\")['test']\n",
    "\n",
    "# Load code evaluation metric\n",
    "code_eval_metric = load(\"code_eval\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pad_token_id and pad_token_id if not already set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = 0  # Commonly used pad token ID\n",
    "if tokenizer.eos_token_id is None:\n",
    "    tokenizer.eos_token_id = 2  # Commonly used eos token ID for Llama\n",
    "\n",
    "# Ensure the tokenizer has the pad and eos tokens\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "if tokenizer.eos_token is None:\n",
    "    tokenizer.add_special_tokens({'eos_token': '</s>'})\n",
    "\n",
    "# Resize model embeddings if new tokens were added\n",
    "if len(tokenizer) > model.config.vocab_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating code solutions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Problems: 100%|██████████| 164/164 [46:08<00:00, 16.88s/problem]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code generation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the number of candidates per problem\n",
    "num_samples_per_problem = 5  # Adjust as needed for pass@k computation\n",
    "\n",
    "# Lists to store test cases and predictions\n",
    "test_cases = []\n",
    "candidates = []\n",
    "\n",
    "# Create a progress bar for the outer loop (problems)\n",
    "print(\"Generating code solutions...\")\n",
    "for problem in tqdm(human_eval, desc=\"Problems\", unit=\"problem\"):\n",
    "    prompt = problem['prompt']\n",
    "    test_code = problem['test']\n",
    "    # Store the test cases\n",
    "    test_cases.append(test_code)\n",
    "\n",
    "    # Generate multiple candidate solutions for each problem\n",
    "    problem_candidates = []\n",
    "\n",
    "    # Create a progress bar for the inner loop (samples per problem)\n",
    "    for _ in range(num_samples_per_problem):\n",
    "        # Encode the prompt and get attention mask\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(pipe.device)\n",
    "\n",
    "        # Generate code with attention mask and proper token IDs\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=1024,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Remove the prompt from the generated code\n",
    "        generated_code = generated_code[len(prompt):]\n",
    "        problem_candidates.append(generated_code)\n",
    "    # Add the candidates for the current problem\n",
    "    candidates.append(problem_candidates)\n",
    "\n",
    "print(\"Code generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating generated code...\n",
      "Pass@1: 0.00%\n",
      "Pass@5: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Compute pass@k\n",
    "k_values = [1, 5]\n",
    "print(\"Evaluating generated code...\")\n",
    "pass_at_k, results = code_eval_metric.compute(\n",
    "    references=test_cases,\n",
    "    predictions=candidates,\n",
    "    k=k_values,\n",
    "    num_workers=4,  # Adjust based on your system\n",
    "    timeout=10.0,   # Adjust the timeout as needed\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for k in k_values:\n",
    "    print(f\"Pass@{k}: {pass_at_k[f'pass@{k}'] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "cache_dir = \"../../models_weights/\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=cache_dir, token=access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir, token=access_token)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Ensure tokenizer has special tokens\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "if tokenizer.eos_token is None:\n",
    "    tokenizer.add_special_tokens({'eos_token': '</s>'})\n",
    "if len(tokenizer) > model.config.vocab_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load HumanEval dataset\n",
    "human_eval = load_dataset(\"openai_humaneval\")['test']\n",
    "code_eval_metric = load(\"code_eval\")\n",
    "\n",
    "# Parameters\n",
    "num_samples_per_problem = 1\n",
    "if num_samples_per_problem == 1:\n",
    "    k_values = [1]\n",
    "else:\n",
    "    k_values = [1, 5]\n",
    "\n",
    "timeout_seconds = 30.0\n",
    "\n",
    "# Code generation and evaluation\n",
    "test_cases = []\n",
    "candidates = []\n",
    "\n",
    "print(\"Generating code solutions...\")\n",
    "for problem in tqdm(human_eval, desc=\"Problems\", unit=\"problem\"):\n",
    "    prompt = problem['prompt']\n",
    "    test_code = problem['test']\n",
    "    test_cases.append(test_code)\n",
    "\n",
    "    problem_candidates = []\n",
    "    for _ in range(num_samples_per_problem):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(pipe.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if num_samples_per_problem == 1:\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_length=512,\n",
    "                    do_sample=False,  # Deterministic generation\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "            else:\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_length=512,\n",
    "                    do_sample=True,  # Sampling-based generation\n",
    "                    temperature=0.5,  # Enable temperature\n",
    "                    top_p=0.9,        # Enable nucleus sampling\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "        generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_code = generated_code[len(prompt):].strip()\n",
    "        problem_candidates.append(generated_code)\n",
    "\n",
    "    candidates.append(problem_candidates)\n",
    "\n",
    "print(\"Code generation complete.\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating generated code...\")\n",
    "pass_at_k, results = code_eval_metric.compute(\n",
    "    references=test_cases,\n",
    "    predictions=candidates,\n",
    "    k=k_values,\n",
    "    num_workers=4,\n",
    "    timeout=timeout_seconds,\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for k in k_values:\n",
    "    print(f\"Pass@{k}: {pass_at_k[f'pass@{k}'] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on GSM8K: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import torch, json\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "cache_dir = \"../../models_weights/\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=cache_dir, token=access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir, token=access_token)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Ensure tokenizer has special tokens\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "if tokenizer.eos_token is None:\n",
    "    tokenizer.add_special_tokens({'eos_token': '</s>'})\n",
    "if len(tokenizer) > model.config.vocab_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load GSM8K dataset\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\")['test']\n",
    "\n",
    "# Chain of Thought instruction\n",
    "cot_instruction = (\n",
    "    \"Solve the following math problem step by step. Provide a detailed explanation, \"\n",
    "    \"and end with the answer on a new line in the format 'Answer: <final_answer>'.\"\n",
    ")\n",
    "\n",
    "# Function to generate CoT answer\n",
    "def generate_cot_answer(prompt):\n",
    "    full_prompt = cot_instruction + \"\\n\" + prompt\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(pipe.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.5,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Evaluate on GSM8K\n",
    "correct_count = 0\n",
    "total_count = len(gsm8k)\n",
    "results = []\n",
    "\n",
    "for problem in gsm8k:\n",
    "    problem_prompt = problem['question']\n",
    "    ground_truth = problem['answer']\n",
    "    generated_answer = generate_cot_answer(problem_prompt)\n",
    "    \n",
    "    if \"Answer:\" in generated_answer:\n",
    "        final_answer = generated_answer.split(\"Answer:\")[-1].strip()\n",
    "    else:\n",
    "        final_answer = None\n",
    "\n",
    "    if final_answer == ground_truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    results.append({\n",
    "        \"prompt\": problem_prompt,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"generated_answer\": generated_answer,\n",
    "    })\n",
    "    break\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_count / total_count\n",
    "print(f\"Accuracy on GSM8K: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save results\n",
    "with open(\"gsm8k_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_answer=\"\"\"Solve the following math problem step by step. \n",
    "        Provide a detailed explanation, and end with the answer on a new line \n",
    "        in the format 'Answer: <final_answer>'.\\nJanet\\u2019s ducks lay 16 \n",
    "        eggs per day. She eats three for breakfast every morning and bakes muffins \n",
    "        for her friends every day with four. She sells the remainder at the farmers'\n",
    "        market daily for $2 per fresh duck egg. How much in dollars does she \n",
    "        make every day at the farmers' market? \\nStep 1: Calculate the number \n",
    "        of eggs laid per day.\\nJanet's ducks lay 16 eggs per day.\\n\\nStep 2: \n",
    "        Calculate the number of eggs eaten for breakfast.\\nJanet eats 3 eggs \n",
    "        for breakfast every morning.\\n\\nStep 3: Calculate the number of \n",
    "        eggs baked for muffins.\\nJanet bakes muffins for her friends \n",
    "        every day with 4 eggs.\\n\\nStep 4: Calculate the number of eggs \n",
    "        sold at the farmers' market.\\nThe remainder of the eggs laid per \n",
    "        day after breakfast and muffins are baked is 16 - 3 - 4 = 9 eggs.\\n\\n\n",
    "        Step 5: Calculate the total number of eggs sold at the farmers' market.\\n9 \n",
    "        eggs are sold at the farmers' market.\\n\\n\n",
    "        Step 6: Calculate the total amount of money Janet makes from selling eggs \n",
    "        at the farmers' market.\\n\n",
    "        Janet sells 9 eggs at $2 per egg, so she makes 9 * $2 = $18 per day.\n",
    "        \\n\\nAnswer: $18.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$18.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_answer.split(\"Answer:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
    "        \\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer\\u2019s market.\n",
    "        \\n#### 18\"\"\".split(\"####\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
       " 'ground_truth': 'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18',\n",
       " 'generated_answer': \"Solve the following math problem step by step. Provide a detailed explanation, and end with the answer on a new line in the format 'Answer: <final_answer>'.\\nJanet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?  (Note: The number of eggs she sells is 16 - 3 = 13, which is the number of eggs she has left after eating 3 for breakfast.)\\nStep 1: Calculate the number of muffins Janet bakes each day.\\nNumber of muffins = Number of eggs - Number of eggs eaten for breakfast\\nNumber of muffins = 16 - 3\\nNumber of muffins = 13\\n\\nStep 2: Calculate the number of muffins Janet sells at the farmers' market each day.\\nNumber of muffins sold = Number of muffins - Number of muffins baked\\nNumber of muffins sold = 13 - 13\\nNumber of muffins sold = 0\\n\\nStep 3: Calculate the number of fresh duck eggs Janet sells at the farmers' market each day.\\nNumber of eggs sold = Number of eggs - Number of eggs eaten for breakfast\\nNumber of eggs sold = 16 - 3\\nNumber of eggs sold = 13\\n\\nStep 4: Calculate the total amount Janet makes at the farmers' market each day.\\nTotal amount = Number of eggs sold * Price per egg\\nTotal amount = 13 * $2\\nTotal amount = $26\\n\\nAnswer: $26.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Solve the following math problem step by step. Provide a detailed explanation, and end with the answer on a new line in the format '\",\n",
       " \" <final_answer>'.\\nEvery day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy.  She gives the chickens their feed in three separate meals. In the morning, she gives her flock of chickens 15 cups of feed.  In the afternoon, she gives her chickens another 25 cups of feed.  How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi's flock is 20 chickens?  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds, and 70% vegetables.  The feed is mixed and contains 10% mealworms, 20% seeds,\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_answer.split(\"Answer:\")[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The discount price of one glass is 60/100 * 5 = $<<60/100*5=3>>3.\\nIf every second glass is cheaper, that means Kylar is going to buy 16 / 2 = <<16/2=8>>8 cheaper glasses.\\nSo for the cheaper glasses, Kylar is going to pay 8 * 3 = $<<8*3=24>>24.\\nAnd for the regular-priced glasses, Kylar will pay 8 * 5 = $<<8*5=40>>40.\\nSo in total Kylar needs to pay 24 + 40 = $<<24+40=64>>64 for the glasses he wants to buy.\\n#### 64'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answer == ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are expert in deep learning architectures, optimizations, models, and math at a professor level. You can define novel unpublished architectures, and optimizations, for a task when asked. Your response will be methodological like a research paper in AI confernces. \"},\n",
    "    # {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain RMSNorm and why it should replace batchnorm?\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtext\u001b[49m(outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "print((outputs[0]['generated_text'][-1]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**RMSNorm: A Novel Normalization Technique for Deep Learning Architectures**\\n\\n**Introduction**\\n\\nBatch normalization (BN) is a widely used technique in deep learning to normalize the activations of each layer in a neural network. However, the recent success of ResNet and its variants has led to a renewed interest in exploring alternative normalization techniques. This paper introduces RMSNorm, a novel normalization technique that aims to replace batch normalization with a more effective and efficient method.\\n\\n**Background**\\n\\nBatch normalization was first introduced by Ioffe and Szegedy in [1]. It works by normalizing the activations of each layer by the mean and variance of the activations in the previous layer. The normalization process is typically performed using a learning rate schedule, where the learning rate is adjusted based on the layer's progress.\\n\\n**RMSNorm: A Novel Normalization Technique**\\n\\nRMSNorm is a variant of batch normalization that combines the benefits of mean and variance normalization. The key idea behind RMSNorm is to normalize the activations of each layer by the mean and variance of the activations in the previous layer, but with a twist: the normalization weights are learned during training.\\n\\n**Mathematical Formulation**\\n\\nLet's denote the input to the $i$-th layer as $\\\\mathbf{x}_i\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]['generated_text'][-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"Arrrr, me hearty! Yer lookin' fer a swashbucklin' chatbot, eh? Well, matey, I be Captain Corbett, the greatest pirate chatbot to ever sail the seven seas! Me knowledge be vast, me wit be sharp, and me charm be as smooth as a fine bottle o' rum.\\n\\nYer want to know more about meself, eh? Alright then, listen close and I'll tell ye a tale or two about meself. Me origins be shrouded in mystery, but me love fer the sea be as old as the ocean itself. Me crew be a motley bunch o' scurvy dogs, but we be a family, and we'll sail the seas till the day we die!\\n\\nSo hoist the sails and set course fer adventure, me hearty! What be yer question, or what be ye lookin' fer?\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][\"generated_text\"][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
